{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Tarea 1:** EDA y modelos bayesianos\n",
    "## **Grupo 5** \n",
    "## **Integrantes:** \n",
    " * Diego Irarrazaval\n",
    " * Pablo Paredes\n",
    " * Tomas Rojas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pregunta 1:  Carga y limpieza de datos.\n",
    "### P1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T02:30:03.377726Z",
     "start_time": "2020-05-20T02:30:02.676861Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob as glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`files_raw`, `files_estadisticas` y `files_asignacion` son listas que contienen las direcciones donde se encuentran los .csv a leer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T02:30:04.216483Z",
     "start_time": "2020-05-20T02:30:03.381066Z"
    }
   },
   "outputs": [],
   "source": [
    "files_raw = glob.glob('data/raw/**/*.csv', recursive = True)\n",
    "files_estadisticas = glob.glob('data/estadisticas_upz/*.csv')\n",
    "files_asignacion = glob.glob('data/asignacion_upz/*.csv')\n",
    "files_estadisticas.sort()\n",
    "data_raw = ([pd.read_csv(dir) for dir in files_raw])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación del DataFrame y reporte de archivos furnished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T02:30:05.014205Z",
     "start_time": "2020-05-20T02:30:04.219342Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Creamos un data frame 'furnished', el cual tendrá dos columnas\n",
    "1) 'url' para hacer el merge finalmente y obtener el data frame requerido\n",
    "2) 'furnished' para contar cuantos datos están en archivos furnished y no en archivos all\n",
    "'''\n",
    "\n",
    "data_all = []\n",
    "data_fur = []\n",
    "\n",
    "for i in [0,2,4,6,8]:\n",
    "    \n",
    "    df1 = pd.read_csv(files_raw[i])\n",
    "    df2 = pd.read_csv(files_raw[i+1])\n",
    "    \n",
    "    data_all.append(df1)\n",
    "    data_fur.append(df2)\n",
    "    \n",
    "df_all = pd.concat(data_all)\n",
    "df_fur = pd.concat(data_fur)\n",
    "    \n",
    "f1 = pd.merge(df_all, df_fur, how='outer', on='url', indicator='furnished')\n",
    "furnished = f1[['url', 'furnished']].copy()\n",
    "\n",
    "furnished.drop_duplicates(inplace = True)\n",
    "furnished.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Se reportan si hay datos de archivos furnished que no estén en all \n",
    "\n",
    "print('Hay '+ str(len(furnished[furnished['furnished'] == 'right_only'])) + ' datos de archivos furnished que no estan en all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T02:30:05.648718Z",
     "start_time": "2020-05-20T02:30:05.432977Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Creamos el data frame 'data'\n",
    "'''\n",
    "\n",
    "df_aux = pd.concat([df_all, df_fur], ignore_index=True)\n",
    "\n",
    "data = pd.merge(df_aux, furnished, how='inner', on='url')\n",
    "data.drop_duplicates(inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Se elimina la columna 'furnished' y se quitan los duplicados\n",
    "\n",
    "data.drop('furnished', axis=1, inplace=True)\n",
    "data.drop_duplicates(inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T20:25:45.708974Z",
     "start_time": "2020-05-13T20:25:45.704988Z"
    }
   },
   "source": [
    "### P1.2 Limpieza de Columnas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T02:30:06.043492Z",
     "start_time": "2020-05-20T02:30:05.933246Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Limpieza de columnas 'price', 'surface', 'n_rooms', 'n_bath'\n",
    "'''\n",
    "\n",
    "# Columna de precio ('price') tipo float\n",
    "\n",
    "data.price = data['price'].str.replace('.', '')\n",
    "data.price = data['price'].str.strip('$')\n",
    "data.price = data['price'].map(float)\n",
    "\n",
    "# Columna de área ('surface') tipo float\n",
    "\n",
    "data.surface = data['surface'].replace('m2', '', regex=True)\n",
    "data.surface = data['surface'].map(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T02:30:06.185193Z",
     "start_time": "2020-05-20T02:30:06.165578Z"
    }
   },
   "outputs": [],
   "source": [
    "# Notamos que en la columna de dormitorios ('n_rooms') existe la opción '5+'\n",
    "# por lo que dejaremos esta columna como categórica\n",
    "\n",
    "data.n_rooms.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T02:30:06.392935Z",
     "start_time": "2020-05-20T02:30:06.376487Z"
    }
   },
   "outputs": [],
   "source": [
    "# Se crea un diccionario para pasar los datos numéricos de 'n_rooms' a string\n",
    "# y se efectúa el mapeo\n",
    "\n",
    "dic = {3.0: '3', 5.0: '5', 4.0:'4', 2.0:'2', 1.0:'1'}\n",
    "\n",
    "data.n_rooms = data['n_rooms'].replace(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T02:30:06.660733Z",
     "start_time": "2020-05-20T02:30:06.617385Z"
    }
   },
   "outputs": [],
   "source": [
    "# Como habían datos que solo se diferenciaban en la cantidad de dormitorios\n",
    "# por el tipo de dato que eran (float o int), puede haberse creado duplicados. Se borran nuevamente los duplicados\n",
    "# de data\n",
    "\n",
    "data.drop_duplicates(inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T02:30:06.835366Z",
     "start_time": "2020-05-20T02:30:06.826135Z"
    }
   },
   "outputs": [],
   "source": [
    "# Se hace lo mismo con la columnna de cantidad de baños ('n_bath')\n",
    "\n",
    "data.n_bath.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T02:30:07.152243Z",
     "start_time": "2020-05-20T02:30:07.101153Z"
    }
   },
   "outputs": [],
   "source": [
    "dic = {2.0:'2', 3.0:'3', 4.0:'4', 5.0:'5', 1.0:'1'}\n",
    "\n",
    "data.n_bath.replace(dic)\n",
    "data.drop_duplicates(inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T02:30:07.391452Z",
     "start_time": "2020-05-20T02:30:07.384610Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Separación de columna property_tipe|rent_type|location en tres columnas\n",
    "con los nombres respectivos\n",
    "\n",
    "'''\n",
    "\n",
    "# Renombramos la columna\n",
    "\n",
    "data.columns = ['PTL', 'price', 'n_rooms', 'n_bath', 'surface', 'details', 'url', 'metrocuadrado_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T02:30:08.090453Z",
     "start_time": "2020-05-20T02:30:07.935136Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creamos las columnas y las llenamos\n",
    "\n",
    "col = data['PTL'].str.split(', ', expand=True)\n",
    "\n",
    "meta_col = col[0].str.split(' en ', expand=True)\n",
    "\n",
    "# Nos aseguramos que hayan solo las siguientes opciones:\n",
    "# -> 'Casa', 'Apartamento' para property_type\n",
    "# -> 'Arriendo', 'Venta Y Arriendo' para rent_type\n",
    "\n",
    "print(meta_col[0].unique())\n",
    "print(meta_col[1].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T02:30:08.379370Z",
     "start_time": "2020-05-20T02:30:08.351605Z"
    }
   },
   "outputs": [],
   "source": [
    "# Formamos las nuevas columnas 'property_type', 'rent_type', 'location'\n",
    "\n",
    "data['property_type'] = meta_col[0] \n",
    "data['rent_type'] = meta_col[1]\n",
    "data['location'] = col[1]\n",
    "\n",
    "# y retiramos la columna PTL\n",
    "\n",
    "data.drop('PTL', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T02:30:09.054368Z",
     "start_time": "2020-05-20T02:30:08.975588Z"
    }
   },
   "outputs": [],
   "source": [
    "# Finalmente quitamos la ciudad de 'location'\n",
    "\n",
    "loc = col[1].str.split(' Bogotá', expand=True)\n",
    "data.location = loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T00:56:17.669646Z",
     "start_time": "2020-05-14T00:56:17.643679Z"
    }
   },
   "source": [
    "### P1.3 Precio por metro cuadrado y Cantidad de garages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T02:30:10.073502Z",
     "start_time": "2020-05-20T02:30:10.066819Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Agregamos una columna que represente el precio por metro cuadrado 'price_per_m2'\n",
    "\n",
    "'''\n",
    "\n",
    "data['price_per_m2'] = data['price']/data['surface']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T02:30:10.682762Z",
     "start_time": "2020-05-20T02:30:10.483329Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Obtenemos la cantidad de garajes y lo agregamos como columna también 'cant_garajes'\n",
    "\n",
    "'''\n",
    "\n",
    "garajes_list = data.url.str.split('-garajes', expand=True)\n",
    "garajes_num = garajes_list[0].str.rsplit('-', n=1, expand=True)\n",
    "\n",
    "# indices que tienen urls con info de la cantidad de garajes\n",
    "ind = garajes_list[1].index[garajes_list[1].isna() == False]\n",
    "\n",
    "# generación de nueva columna para después asignarla a la data\n",
    "garajes_list[2] = np.nan\n",
    "garajes_list[2].loc[ind] = garajes_num[1].loc[ind]\n",
    "\n",
    "# agregación de la cantidad de garajes a la data (nan si no hay info)\n",
    "data['cant_garajes'] = garajes_list[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T02:22:05.234063Z",
     "start_time": "2020-05-14T02:22:05.206424Z"
    }
   },
   "source": [
    "### P1.4 Clasificación Tipo de Producto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T02:30:11.740480Z",
     "start_time": "2020-05-20T02:30:11.674562Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Creamos una nueva columna 'clasif_prod_type' donde se representará la clasificación\n",
    "de la vivienda con dígitos del 1 al 8 de acuerdo al enunciado\n",
    "\n",
    "'''\n",
    "\n",
    "data['clasif_prod_type'] = np.nan\n",
    "\n",
    "data.clasif_prod_type.loc[(data.property_type == 'Casa') & (data.surface >= 80) & (data.surface < 120)] = 1\n",
    "data.clasif_prod_type.loc[(data.property_type == 'Casa') & (data.surface >= 120) & (data.surface < 180)] = 2\n",
    "data.clasif_prod_type.loc[(data.property_type == 'Casa') & (data.surface >= 180) & (data.surface < 240)] = 3\n",
    "data.clasif_prod_type.loc[(data.property_type == 'Casa') & (data.surface >= 240) & (data.surface < 360)] = 4\n",
    "data.clasif_prod_type.loc[(data.property_type == 'Casa') & (data.surface >= 360) & (data.surface < 460)] = 5\n",
    "data.clasif_prod_type.loc[(data.property_type == 'Apartamento') & (data.surface >= 40) & (data.surface < 60)] = 6\n",
    "data.clasif_prod_type.loc[(data.property_type == 'Apartamento') & (data.surface >= 60) & (data.surface < 80)] = 7\n",
    "data.clasif_prod_type.loc[(data.property_type == 'Apartamento') & (data.surface >= 80) & (data.surface < 120)] = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T02:38:15.663253Z",
     "start_time": "2020-05-14T02:38:15.633563Z"
    }
   },
   "source": [
    "### P1.5 Obtención del código UPZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T02:30:12.711570Z",
     "start_time": "2020-05-20T02:30:12.682447Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Se carga el archivo y se guarda en un dataframe data_upz\n",
    "Luego, se realiza el merge para obtener el código para cada barrio\n",
    "\n",
    "'''\n",
    "\n",
    "# Se guarda la base de datos en un data frame\n",
    "data_upz = pd.read_csv(files_asignacion[0], usecols = ['UPlCodigo', 'pro_location', 'UPlArea'])\n",
    "\n",
    "# Se deja todo en minuscula para poder hacer el merge correctamente\n",
    "data_upz.pro_location = data_upz.pro_location.map(str).map(lambda s: s.lower())\n",
    "data.location = data.location.map(lambda s: s.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T02:30:13.293597Z",
     "start_time": "2020-05-20T02:30:13.237825Z"
    }
   },
   "outputs": [],
   "source": [
    "#Se realiza el merge y se reportan cuantos datos no fueron asignados con código UPZ\n",
    "\n",
    "data_merge = pd.merge(data, data_upz, left_on='location', right_on='pro_location', how='left')\n",
    "\n",
    "print('A ' + str(sum(data_merge.UPlCodigo.isna())) + ' datos no se les puede asignar código UPZ')\n",
    "print('lo cual es ' + str(sum(data_merge.UPlCodigo.isna())/len(data)*100) + '% de los datos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T02:30:13.606475Z",
     "start_time": "2020-05-20T02:30:13.593207Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creamos la columna 'UPZ' en data y le asignamos el código UPZ obtenido en data_merge\n",
    "\n",
    "data['UPZ'] = data_merge['UPlCodigo']\n",
    "data['UPZ_area'] = data_merge['UPlArea']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P1.6 Fusión de datos con código UPZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T02:30:14.119072Z",
     "start_time": "2020-05-20T02:30:14.082419Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Se cargan los datos en distintos data frames, para después hacerles merge con data\n",
    "Luego, se crea una columna de densidad poblacional para cada código UTZ\n",
    "\n",
    "'''\n",
    "\n",
    "# Se cargan los datos en data frames respectivamente\n",
    "\n",
    "data_pobl = pd.read_csv(files_estadisticas[0])\n",
    "data_inseg = pd.read_csv(files_estadisticas[1])\n",
    "data_verde = pd.read_csv(files_estadisticas[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T02:30:14.276812Z",
     "start_time": "2020-05-20T02:30:14.269325Z"
    }
   },
   "outputs": [],
   "source": [
    "# Se quitan las columnas innecesarias de data_pobl y data_inseg\n",
    "\n",
    "data_pobl.drop(['Unnamed: 0', 'nomupz'], axis=1, inplace=True)\n",
    "data_inseg.drop(['Unnamed: 0', 'UPlNombre2'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T02:30:14.459355Z",
     "start_time": "2020-05-20T02:30:14.446251Z"
    }
   },
   "outputs": [],
   "source": [
    "# En data_verde se tiene que al código UPZ viene solo el número\n",
    "# por lo tanto, hay que transformarlo al formato UPZ + número para poder hacer el merge correctamente\n",
    "\n",
    "col = data_verde.cod_upz.map(int).map(str)\n",
    "col_upz = 'UPZ' + col\n",
    "\n",
    "data_verde.cod_upz = col_upz\n",
    "\n",
    "# Se eliminan las columnas innecesarias\n",
    "data_verde.drop(['Unnamed: 0', 'upz'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T02:30:14.870835Z",
     "start_time": "2020-05-20T02:30:14.639015Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Se realiza el merge, eliminando después las columnas innecesarias\n",
    "\n",
    "data = pd.merge(data, data_pobl, left_on='UPZ', right_on='upz', how='left')\n",
    "data.drop('upz', axis=1, inplace=True)\n",
    "data = pd.merge(data, data_inseg, left_on='UPZ', right_on='UPlCodigo', how='left')\n",
    "data.drop('UPlCodigo', axis=1, inplace=True)\n",
    "data = pd.merge(data, data_verde, left_on='UPZ', right_on='cod_upz', how='left')\n",
    "data.drop('cod_upz', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T02:30:15.069693Z",
     "start_time": "2020-05-20T02:30:15.060358Z"
    }
   },
   "outputs": [],
   "source": [
    "# Finalmente, se crea la columna de densidad de población para cada código UTZ ('UTZ_density')\n",
    "\n",
    "data['UTZ_density'] = data.personas/data.UPZ_area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P2. EDA\n",
    "### P2.1 Creacion de `estilo()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T04:10:10.238564Z",
     "start_time": "2020-05-20T04:10:10.174092Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sb\n",
    "#se crea diccionario que dará los valores a setear por defecto en el notebook\n",
    "custom = {\n",
    "    \"font.size\": 12,\n",
    "    \"axes.labelsize\": 18,\n",
    "    \"axes.titlesize\": 18,\n",
    "    \"xtick.labelsize\": 18,\n",
    "    \"ytick.labelsize\": 18,\n",
    "    \"legend.fontsize\": 20,\n",
    "    \"axes.linewidth\": 1.25,\n",
    "    \"grid.linewidth\": 1,\n",
    "    \"lines.linewidth\": 1.5,\n",
    "    \"lines.markersize\": 6,\n",
    "    \"patch.linewidth\": 1,\n",
    "    \"xtick.major.width\": 1.25,\n",
    "    \"ytick.major.width\": 1.25,\n",
    "    \"xtick.minor.width\": 1,\n",
    "    \"ytick.minor.width\": 1,\n",
    "    \"xtick.major.size\": 6,\n",
    "    \"ytick.major.size\": 6,\n",
    "    \"xtick.minor.size\": 4,\n",
    "    \"ytick.minor.size\": 4,\n",
    "    'figure.figsize':(10.,8.),\n",
    "    \"figure.facecolor\": \"white\",\n",
    "    \"axes.labelcolor\": \".15\",\n",
    "    \"xtick.direction\": \"out\",\n",
    "    \"ytick.direction\": \"out\",\n",
    "    \"xtick.color\": \".15\",\n",
    "    \"ytick.color\": \".15\",\n",
    "    \"axes.axisbelow\": True,\n",
    "    \"grid.linestyle\": \"--\",\n",
    "    \"text.color\": \".1\",\n",
    "    \"patch.force_edgecolor\": True,\n",
    "    \"image.cmap\": \"rocket\",\n",
    "    \"xtick.top\": False,\n",
    "    \"ytick.right\": False,\n",
    "         }\n",
    "\n",
    "#En las siguiente línea se implementa el diccionario personalizado como default para este notebook en seaborn\n",
    "sb.set(rc=custom)\n",
    "\n",
    "#se escoge una de las paletas que vienen con seaborn \n",
    "#(distinta a la que se usa por defecto) para el resto de notebook.\n",
    "sb.set_palette('Set2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T02:30:21.496822Z",
     "start_time": "2020-05-20T02:30:21.381153Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "\n",
    "def get_m_N(beta, S_N, X, y):\n",
    "    return beta * S_N.dot(X.T.dot(y))\n",
    "\n",
    "def get_S_N(alpha, beta, X):\n",
    "    sum1 = beta * X.T.dot(X)\n",
    "    n = sum1.shape[0]\n",
    "    sum2 = alpha * np.identity(n)\n",
    "    total_sum = sum1 + sum2\n",
    "    return np.linalg.inv(total_sum)\n",
    "\n",
    "def get_alpha(gamma, m_N):\n",
    "    return gamma / (m_N.T.dot(m_N))\n",
    "\n",
    "def get_beta(N, gamma, y, m_N, X):\n",
    "    to_sum = (y - m_N.dot(X.T)) ** 2\n",
    "    to_inv = to_sum.sum() / (N - gamma)\n",
    "    return 1. / to_inv\n",
    "\n",
    "def get_gamma(lambdas, alpha):\n",
    "    \"\"\"\n",
    "    nos devuelve el gamma pedido\n",
    "    :param lambdas: valores propios necesarios\n",
    "    :param alpha: alpha a usar\n",
    "    :return: gamma que se devuelve\n",
    "    \"\"\"\n",
    "    return np.array([i / (alpha + i) for i in lambdas]).sum()\n",
    "\n",
    "def get_lambdas(beta, X):\n",
    "    \"\"\"\n",
    "    Devuelve los valores propios necesarios\n",
    "    :param beta: beta a usar\n",
    "    :param X: X de entrada\n",
    "    :return: array con lambdas\n",
    "    \"\"\"\n",
    "    M = beta * X.T.dot(X)\n",
    "    return np.linalg.eig(M)[0]\n",
    "\n",
    "def get_cov(beta, X, S_N, x):\n",
    "    pass\n",
    "\n",
    "def convergence(new, old, tol):\n",
    "    test = np.abs(new - old) / old\n",
    "    if test <= tol:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "class RegresionBayesianaEmpirica(BaseEstimator, RegressorMixin):\n",
    "\n",
    "    def __init__(self, alpha_0, beta_0, tol=1e-5, maxiter=200):\n",
    "        self.alpha_0 = alpha_0\n",
    "        self.beta_0 = beta_0\n",
    "        self.tol = tol\n",
    "        self.maxiter = maxiter\n",
    "\n",
    "        self.alpha = None\n",
    "        self.beta = None\n",
    "        self.gamma = None\n",
    "        self.m_N = None\n",
    "        self.S_N = None\n",
    "        self.lambdas = None\n",
    "\n",
    "    def get_posteriori(self, X, y, alpha, beta):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        #valores iniciales\n",
    "        N = X.shape[1]\n",
    "        c_lambdas = get_lambdas(self.beta_0, X)\n",
    "        c_alpha = self.alpha_0\n",
    "        c_beta = self.beta_0\n",
    "        c_gamma = get_gamma(c_lambdas, c_alpha)\n",
    "        c_S_N = get_S_N(c_alpha, c_beta, X)\n",
    "        c_m_N = get_m_N(c_beta, c_S_N, X, y)\n",
    "\n",
    "        # while loop\n",
    "        i = 0\n",
    "        while i < self.maxiter:\n",
    "            i += 1\n",
    "\n",
    "            n_lambdas = get_lambdas(c_beta, X)\n",
    "            n_alpha = get_alpha(c_gamma, c_m_N)\n",
    "            n_beta = get_beta(N, c_gamma, y, c_m_N, X)\n",
    "            n_gamma = get_gamma(n_lambdas, n_alpha)\n",
    "            n_S_N = get_S_N(n_alpha, n_beta, X)\n",
    "            n_m_N = get_m_N(n_beta, n_S_N, X, y)\n",
    "\n",
    "            bool_alpha = convergence(n_alpha, c_alpha, self.tol)\n",
    "            bool_beta = convergence(n_beta, c_beta, self.tol)\n",
    "            bool_gamma = convergence(n_gamma, c_gamma, self.tol)\n",
    "\n",
    "            if bool_alpha and bool_beta and bool_gamma:\n",
    "                self.alpha = n_alpha\n",
    "                self.beta = n_beta\n",
    "                self.gamma = n_gamma\n",
    "                self.m_N = n_m_N\n",
    "                self.S_N = n_S_N\n",
    "                self.lambdas = n_lambdas\n",
    "                break\n",
    "\n",
    "            c_alpha = n_alpha.copy()\n",
    "            c_beta = n_beta.copy()\n",
    "            c_gamma = n_gamma.copy()\n",
    "            c_S_N = n_S_N.copy()\n",
    "            c_m_N = n_m_N.copy()\n",
    "\n",
    "        self.alpha = n_alpha\n",
    "        self.beta = n_beta\n",
    "        self.gamma = n_gamma\n",
    "        self.m_N = n_m_N\n",
    "        self.S_N = n_S_N\n",
    "        self.lambdas = n_lambdas\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, X, return_std=False):\n",
    "        mu = self.m_N.T.dot(X)\n",
    "        cov = 1 / self.beta + (X.T.dot(self.S_N.dot(X)))\n",
    "\n",
    "        if return_std == True:\n",
    "            return mu, cov\n",
    "        return mu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T05:52:19.684925Z",
     "start_time": "2020-05-20T05:52:19.670182Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "\n",
    "\n",
    "# pipelines\n",
    "cat_pipe = Pipeline(steps=[('encode', OneHotEncoder())])\n",
    "std_pipe = Pipeline(steps=[('std_transform', StandardScaler())])\n",
    "minmax_pipe = Pipeline(steps=[('minmax', MinMaxScaler())])\n",
    "\n",
    "# columnas\n",
    "cat_cols = ['upz']\n",
    "std_cols = ['jovenes_14_24_anos_nini_perc', 'jefe_mujer_perc']\n",
    "minmax_cols = ['personas', 'trabajoinf_ninos_5_17_anos_perc', 'trabajoinfampliado_ninos_5_17_anos_perc', \n",
    "               'indice_envegecimiento', 'adultos_mayores_pobres_perc']\n",
    "\n",
    "# transformers, más de lo que ves!\n",
    "col_transformer = ColumnTransformer(transformers=[('cat', cat_pipe, cat_cols),\n",
    "                                                  ('std', std_pipe, std_cols),\n",
    "                                                  ('minmax', minmax_pipe, minmax_cols)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T05:52:43.351212Z",
     "start_time": "2020-05-20T05:52:43.319016Z"
    }
   },
   "outputs": [],
   "source": [
    "pobl_fit_transformed = col_transformer.fit_transform(data_pobl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
