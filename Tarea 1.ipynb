{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Tarea 1:** EDA y modelos bayesianos\n",
    "## **Grupo 5** \n",
    "## **Integrantes:** \n",
    " * Diego Irarrazaval\n",
    " * Pablo Paredes\n",
    " * Tomas Rojas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pregunta 1:  Carga y limpieza de datos.\n",
    "### P1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T04:18:05.812793Z",
     "start_time": "2020-05-14T04:18:04.987869Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob as glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`files_raw`, `files_estadisticas` y `files_asignacion` son listas que contienen las direcciones donde se encuentran los .csv a leer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T04:18:06.304004Z",
     "start_time": "2020-05-14T04:18:05.814738Z"
    }
   },
   "outputs": [],
   "source": [
    "files_raw = glob.glob('data/raw/**/*.csv', recursive = True)\n",
    "files_estadisticas = glob.glob('data/estadisticas_upz/*.csv')\n",
    "files_asignacion = glob.glob('data/asignacion_upz/*.csv')\n",
    "\n",
    "data_raw = ([pd.read_csv(dir) for dir in files_raw])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación del DataFrame y reporte de archivos furnished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T04:18:08.214800Z",
     "start_time": "2020-05-14T04:18:06.306002Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Creamos un data frame 'furnished', el cual tendrá dos columnas\n",
    "1) 'url' para hacer el merge finalmente y obtener el data frame requerido\n",
    "2) 'furnished' para contar cuantos datos están en archivos furnished y no en archivos all\n",
    "'''\n",
    "\n",
    "data_all = []\n",
    "data_fur = []\n",
    "\n",
    "for i in [0,2,4,6,8]:\n",
    "    \n",
    "    df1 = pd.read_csv(files_raw[i])\n",
    "    df2 = pd.read_csv(files_raw[i+1])\n",
    "    \n",
    "    data_all.append(df1)\n",
    "    data_fur.append(df2)\n",
    "    \n",
    "df_all = pd.concat(data_all)\n",
    "df_fur = pd.concat(data_fur)\n",
    "    \n",
    "f1 = pd.merge(df_all, df_fur, how='outer', on='url', indicator='furnished')\n",
    "furnished = f1[['url', 'furnished']].copy()\n",
    "\n",
    "furnished.drop_duplicates(inplace = True)\n",
    "furnished.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Se reportan si hay datos de archivos furnished que no estén en all \n",
    "\n",
    "print('Hay '+ str(len(furnished[furnished['furnished'] == 'right_only'])) + ' datos de archivos furnished que no estan en all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T04:18:08.413806Z",
     "start_time": "2020-05-14T04:18:08.216770Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Creamos el data frame 'data'\n",
    "'''\n",
    "\n",
    "df_aux = pd.concat([df_all, df_fur], ignore_index=True)\n",
    "\n",
    "data = pd.merge(df_aux, furnished, how='inner', on='url')\n",
    "data.drop_duplicates(inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Se elimina la columna 'furnished' y se quitan los duplicados\n",
    "\n",
    "data.drop('furnished', axis=1, inplace=True)\n",
    "data.drop_duplicates(inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T20:25:45.708974Z",
     "start_time": "2020-05-13T20:25:45.704988Z"
    }
   },
   "source": [
    "### P1.2 Limpieza de Columnas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T04:18:09.679075Z",
     "start_time": "2020-05-14T04:18:08.415794Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Limpieza de columnas 'price', 'surface', 'n_rooms', 'n_bath'\n",
    "'''\n",
    "\n",
    "# Columna de precio ('price') tipo float\n",
    "\n",
    "data.price = data['price'].str.replace('.', '')\n",
    "data.price = data['price'].str.strip('$')\n",
    "data.price = data['price'].map(float)\n",
    "\n",
    "# Columna de área ('surface') tipo float\n",
    "\n",
    "data.surface = data['surface'].replace('m2', '', regex=True)\n",
    "data.surface = data['surface'].map(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T04:18:09.817704Z",
     "start_time": "2020-05-14T04:18:09.680847Z"
    }
   },
   "outputs": [],
   "source": [
    "# Notamos que en la columna de dormitorios ('n_rooms') existe la opción '5+'\n",
    "# por lo que dejaremos esta columna como categórica\n",
    "\n",
    "data.n_rooms.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T04:18:10.016283Z",
     "start_time": "2020-05-14T04:18:09.819690Z"
    }
   },
   "outputs": [],
   "source": [
    "# Se crea un diccionario para pasar los datos numéricos de 'n_rooms' a string\n",
    "# y se efectúa el mapeo\n",
    "\n",
    "dic = {3.0: '3', 5.0: '5', 4.0:'4', 2.0:'2', 1.0:'1'}\n",
    "\n",
    "data.n_rooms = data['n_rooms'].replace(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T04:18:10.180746Z",
     "start_time": "2020-05-14T04:18:10.023026Z"
    }
   },
   "outputs": [],
   "source": [
    "# Como habían datos que solo se diferenciaban en la cantidad de dormitorios\n",
    "# por el tipo de dato que eran (float o int), puede haberse creado duplicados. Se borran nuevamente los duplicados\n",
    "# de data\n",
    "\n",
    "data.drop_duplicates(inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T04:18:10.263328Z",
     "start_time": "2020-05-14T04:18:10.182707Z"
    }
   },
   "outputs": [],
   "source": [
    "# Se hace lo mismo con la columnna de cantidad de baños ('n_bath')\n",
    "\n",
    "data.n_bath.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T04:18:10.469269Z",
     "start_time": "2020-05-14T04:18:10.265343Z"
    }
   },
   "outputs": [],
   "source": [
    "dic = {2.0:'2', 3.0:'3', 4.0:'4', 5.0:'5', 1.0:'1'}\n",
    "\n",
    "data.n_bath.replace(dic)\n",
    "data.drop_duplicates(inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T04:18:10.544855Z",
     "start_time": "2020-05-14T04:18:10.471210Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Separación de columna property_tipe|rent_type|location en tres columnas\n",
    "con los nombres respectivos\n",
    "\n",
    "'''\n",
    "\n",
    "# Renombramos la columna\n",
    "\n",
    "data.columns = ['PTL', 'price', 'n_rooms', 'n_bath', 'surface', 'details', 'url', 'metrocuadrado_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T04:18:10.732619Z",
     "start_time": "2020-05-14T04:18:10.546562Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creamos las columnas y las llenamos\n",
    "\n",
    "col = data['PTL'].str.split(', ', expand=True)\n",
    "\n",
    "meta_col = col[0].str.split(' en ', expand=True)\n",
    "\n",
    "# Nos aseguramos que hayan solo las siguientes opciones:\n",
    "# -> 'Casa', 'Apartamento' para property_type\n",
    "# -> 'Arriendo', 'Venta Y Arriendo' para rent_type\n",
    "\n",
    "print(meta_col[0].unique())\n",
    "print(meta_col[1].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T04:18:10.811432Z",
     "start_time": "2020-05-14T04:18:10.734619Z"
    }
   },
   "outputs": [],
   "source": [
    "# Formamos las nuevas columnas 'property_type', 'rent_type', 'location'\n",
    "\n",
    "data['property_type'] = meta_col[0] \n",
    "data['rent_type'] = meta_col[1]\n",
    "data['location'] = col[1]\n",
    "\n",
    "# y retiramos la columna PTL\n",
    "\n",
    "data.drop('PTL', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T04:18:10.953442Z",
     "start_time": "2020-05-14T04:18:10.813089Z"
    }
   },
   "outputs": [],
   "source": [
    "# Finalmente quitamos la ciudad de 'location'\n",
    "\n",
    "loc = col[1].str.split(' Bogotá', expand=True)\n",
    "data.location = loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T00:56:17.669646Z",
     "start_time": "2020-05-14T00:56:17.643679Z"
    }
   },
   "source": [
    "### P1.3 Precio por metro cuadrado y Cantidad de garages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T04:18:11.389470Z",
     "start_time": "2020-05-14T04:18:10.955051Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Agregamos una columna que represente el precio por metro cuadrado 'price_per_m2'\n",
    "\n",
    "'''\n",
    "\n",
    "data['price_per_m2'] = data['price']/data['surface']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T04:18:11.659586Z",
     "start_time": "2020-05-14T04:18:11.392310Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Obtenemos la cantidad de garajes y lo agregamos como columna también 'cant_garajes'\n",
    "\n",
    "'''\n",
    "\n",
    "garajes_list = data.url.str.split('-garajes', expand=True)\n",
    "garajes_num = garajes_list[0].str.rsplit('-', n=1, expand=True)\n",
    "\n",
    "# indices que tienen urls con info de la cantidad de garajes\n",
    "ind = garajes_list[1].index[garajes_list[1].isna() == False]\n",
    "\n",
    "# generación de nueva columna para después asignarla a la data\n",
    "garajes_list[2] = np.nan\n",
    "garajes_list[2].loc[ind] = garajes_num[1].loc[ind]\n",
    "\n",
    "# agregación de la cantidad de garajes a la data (nan si no hay info)\n",
    "data['cant_garajes'] = garajes_list[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T02:22:05.234063Z",
     "start_time": "2020-05-14T02:22:05.206424Z"
    }
   },
   "source": [
    "### P1.4 Clasificación Tipo de Producto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T04:18:11.723221Z",
     "start_time": "2020-05-14T04:18:11.661866Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Creamos una nueva columna 'clasif_prod_type' donde se representará la clasificación\n",
    "de la vivienda con dígitos del 1 al 8 de acuerdo al enunciado\n",
    "\n",
    "'''\n",
    "\n",
    "data['clasif_prod_type'] = np.nan\n",
    "\n",
    "data.clasif_prod_type.loc[(data.property_type == 'Casa') & (data.surface >= 80) & (data.surface < 120)] = 1\n",
    "data.clasif_prod_type.loc[(data.property_type == 'Casa') & (data.surface >= 120) & (data.surface < 180)] = 2\n",
    "data.clasif_prod_type.loc[(data.property_type == 'Casa') & (data.surface >= 180) & (data.surface < 240)] = 3\n",
    "data.clasif_prod_type.loc[(data.property_type == 'Casa') & (data.surface >= 240) & (data.surface < 360)] = 4\n",
    "data.clasif_prod_type.loc[(data.property_type == 'Casa') & (data.surface >= 360) & (data.surface < 460)] = 5\n",
    "data.clasif_prod_type.loc[(data.property_type == 'Apartamento') & (data.surface >= 40) & (data.surface < 60)] = 6\n",
    "data.clasif_prod_type.loc[(data.property_type == 'Apartamento') & (data.surface >= 60) & (data.surface < 80)] = 7\n",
    "data.clasif_prod_type.loc[(data.property_type == 'Apartamento') & (data.surface >= 80) & (data.surface < 120)] = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T02:38:15.663253Z",
     "start_time": "2020-05-14T02:38:15.633563Z"
    }
   },
   "source": [
    "### P1.5 Obtención del código UPZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T04:18:11.802744Z",
     "start_time": "2020-05-14T04:18:11.724221Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Se carga el archivo y se guarda en un dataframe data_upz\n",
    "Luego, se realiza el merge para obtener el código para cada barrio\n",
    "\n",
    "'''\n",
    "\n",
    "# Se guarda la base de datos en un data frame\n",
    "data_upz = pd.read_csv(files_asignacion[0], usecols = ['UPlCodigo', 'pro_location', 'UPlArea'])\n",
    "\n",
    "# Se deja todo en minuscula para poder hacer el merge correctamente\n",
    "data_upz.pro_location = data_upz.pro_location.map(str).map(lambda s: s.lower())\n",
    "data.location = data.location.map(lambda s: s.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T04:18:11.974600Z",
     "start_time": "2020-05-14T04:18:11.803742Z"
    }
   },
   "outputs": [],
   "source": [
    "#Se realiza el merge y se reportan cuantos datos no fueron asignados con código UPZ\n",
    "\n",
    "data_merge = pd.merge(data, data_upz, left_on='location', right_on='pro_location', how='left')\n",
    "\n",
    "print('A ' + str(sum(data_merge.UPlCodigo.isna())) + ' datos no se les puede asignar código UPZ')\n",
    "print('lo cual es ' + str(sum(data_merge.UPlCodigo.isna())/len(data)*100) + '% de los datos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T04:18:12.044238Z",
     "start_time": "2020-05-14T04:18:11.976582Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creamos la columna 'UPZ' en data y le asignamos el código UPZ obtenido en data_merge\n",
    "\n",
    "data['UPZ'] = data_merge['UPlCodigo']\n",
    "data['UPZ_area'] = data_merge['UPlArea']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P1.6 Fusión de datos con código UPZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T04:18:12.167936Z",
     "start_time": "2020-05-14T04:18:12.049016Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Se cargan los datos en distintos data frames, para después hacerles merge con data\n",
    "Luego, se crea una columna de densidad poblacional para cada código UTZ\n",
    "\n",
    "'''\n",
    "\n",
    "# Se cargan los datos en data frames respectivamente\n",
    "\n",
    "data_pobl = pd.read_csv(files_estadisticas[0])\n",
    "data_inseg = pd.read_csv(files_estadisticas[1])\n",
    "data_verde = pd.read_csv(files_estadisticas[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T04:18:12.255661Z",
     "start_time": "2020-05-14T04:18:12.170692Z"
    }
   },
   "outputs": [],
   "source": [
    "# Se quitan las columnas innecesarias de data_pobl y data_inseg\n",
    "\n",
    "data_pobl.drop(['Unnamed: 0', 'nomupz'], axis=1, inplace=True)\n",
    "data_inseg.drop(['Unnamed: 0', 'UPlNombre2'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T04:18:12.368191Z",
     "start_time": "2020-05-14T04:18:12.264637Z"
    }
   },
   "outputs": [],
   "source": [
    "# En data_verde se tiene que al código UPZ viene solo el número\n",
    "# por lo tanto, hay que transformarlo al formato UPZ + número para poder hacer el merge correctamente\n",
    "\n",
    "col = data_verde.cod_upz.map(int).map(str)\n",
    "col_upz = 'UPZ' + col\n",
    "\n",
    "data_verde.cod_upz = col_upz\n",
    "\n",
    "# Se eliminan las columnas innecesarias\n",
    "data_verde.drop(['Unnamed: 0', 'upz'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T04:18:12.700467Z",
     "start_time": "2020-05-14T04:18:12.370166Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Se realiza el merge, eliminando después las columnas innecesarias\n",
    "\n",
    "data = pd.merge(data, data_pobl, left_on='UPZ', right_on='upz', how='left')\n",
    "data.drop('upz', axis=1, inplace=True)\n",
    "data = pd.merge(data, data_inseg, left_on='UPZ', right_on='UPlCodigo', how='left')\n",
    "data.drop('UPlCodigo', axis=1, inplace=True)\n",
    "data = pd.merge(data, data_verde, left_on='UPZ', right_on='cod_upz', how='left')\n",
    "data.drop('cod_upz', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T04:21:21.225400Z",
     "start_time": "2020-05-14T04:21:21.220386Z"
    }
   },
   "outputs": [],
   "source": [
    "# Finalmente, se crea la columna de densidad de población para cada código UTZ ('UTZ_density')\n",
    "\n",
    "data['UTZ_density'] = data.personas/data.UPZ_area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P2. EDA\n",
    "### P2.1 Creacion de `estilo()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sb\n",
    "#se crea diccionario que dará los valores a setear por defecto en el notebook\n",
    "custom = {\n",
    "    \"font.size\": 12,\n",
    "    \"axes.labelsize\": 18,\n",
    "    \"axes.titlesize\": 18,\n",
    "    \"xtick.labelsize\": 18,\n",
    "    \"ytick.labelsize\": 18,\n",
    "    \"legend.fontsize\": 20,\n",
    "    \"axes.linewidth\": 1.25,\n",
    "    \"grid.linewidth\": 1,\n",
    "    \"lines.linewidth\": 1.5,\n",
    "    \"lines.markersize\": 6,\n",
    "    \"patch.linewidth\": 1,\n",
    "    \"xtick.major.width\": 1.25,\n",
    "    \"ytick.major.width\": 1.25,\n",
    "    \"xtick.minor.width\": 1,\n",
    "    \"ytick.minor.width\": 1,\n",
    "    \"xtick.major.size\": 6,\n",
    "    \"ytick.major.size\": 6,\n",
    "    \"xtick.minor.size\": 4,\n",
    "    \"ytick.minor.size\": 4,\n",
    "    'figure.figsize':(10.,8.),\n",
    "    \"figure.facecolor\": \"white\",\n",
    "    \"axes.labelcolor\": \".15\",\n",
    "    \"xtick.direction\": \"out\",\n",
    "    \"ytick.direction\": \"out\",\n",
    "    \"xtick.color\": \".15\",\n",
    "    \"ytick.color\": \".15\",\n",
    "    \"axes.axisbelow\": True,\n",
    "    \"grid.linestyle\": \"--\",\n",
    "    \"text.color\": \".1\",\n",
    "    \"patch.force_edgecolor\": True,\n",
    "    \"image.cmap\": \"rocket\",\n",
    "    \"xtick.top\": False,\n",
    "    \"ytick.right\": False,\n",
    "         }\n",
    "\n",
    "#En las siguiente línea se implementa el diccionario personalizado como default para este notebook en seaborn\n",
    "sb.set(rc=custom)\n",
    "\n",
    "#se escoge una de las paletas que vienen con seaborn \n",
    "#(distinta a la que se usa por defecto) para el resto de notebook.\n",
    "sb.set_palette('Set2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T20:24:10.037088Z",
     "start_time": "2020-05-16T20:24:10.012330Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "def convergence(old, new, tol):\n",
    "    test = np.abs(new - old) / old\n",
    "    if test <= tol:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def calculate_eigs(beta, X):\n",
    "    M = beta * np.matmul(X.T, X)\n",
    "    return np.linalg.eig(M)[0]\n",
    "\n",
    "def get_S_N(alpha, beta, X):\n",
    "    a = beta * np.matmul(X.T, X)\n",
    "    b = alpha * np.identity(a.shape)\n",
    "    to_return = a + b\n",
    "    return np.linalg.inv(to_return)\n",
    "\n",
    "\n",
    "def get_m_N(beta, S_N, X, y):\n",
    "    return beta * S_N.dot(X.T.dot(y))\n",
    "\n",
    "\n",
    "def get_alpha(gamma, m_N):\n",
    "    return gamma / np.matmul(m_N.T, m_N)\n",
    "\n",
    "\n",
    "def get_beta(N, gamma, m_N, X, y):\n",
    "    \"\"\"\n",
    "    Funcion que nos da beta para una iteracion dada\n",
    "    :param N:\n",
    "    :param gamma: parametro gamma\n",
    "    :param m_N: matriz\n",
    "    :param X: matrix con los vectores fila x_i uno sobre el otro\n",
    "    :param y: vector con las salidas\n",
    "    :return: devuelve un valor para beta\n",
    "    \"\"\"\n",
    "    print(m_N.T.shape, X.shape)\n",
    "    m_N_X = np.matmul(m_N.T, X.T)\n",
    "    suma = y - m_N_X\n",
    "    suma = suma ** 2\n",
    "    return (suma.sum(axis=0) / (N - gamma)) ** -1\n",
    "\n",
    "\n",
    "def get_gamma(alpha, beta, X):\n",
    "    eig = calculate_eigs(beta, X)\n",
    "    terms = np.array([i / (alpha + i) for i in eig])\n",
    "    return terms.sum()\n",
    "\n",
    "\n",
    "class RegresionBayesianaEmpirica(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, alpha_0, beta_0, tol=1e-5, maxiter=200):\n",
    "        \"\"\"\n",
    "        :param alpha_0: initial guess for alpha\n",
    "        :param beta_0: initial guess for beta\n",
    "        :param tol: tolerance\n",
    "        :param maxiter: maximum iterations\n",
    "        \"\"\"\n",
    "        # valores iniciales\n",
    "        self.alpha_0 = alpha_0\n",
    "        self.beta_0 = beta_0\n",
    "\n",
    "        # propiedades de convergencia\n",
    "        self.tol = tol\n",
    "        self.maxiter = maxiter\n",
    "\n",
    "        self.alpha = None\n",
    "        self.beta = None\n",
    "        self.gamma = None\n",
    "\n",
    "        self.posteriori = None\n",
    "\n",
    "\n",
    "\n",
    "    def get_posteriori(self, X, y, alpha, beta):\n",
    "        \"\"\"\n",
    "        Esta func tiene que darnos la dist posterior\n",
    "        :param X:\n",
    "        :param y:\n",
    "        :param alpha:\n",
    "        :param beta:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        S_N = get_S_N(alpha, beta, X)\n",
    "        m_N = get_m_N(beta, S_N, X, y)\n",
    "        mu = m_N.T.dot(X)\n",
    "        var = lambda x: (1 / beta) + X.T.dot(S_N.dot(x))\n",
    "        self.posteriori = lambda x: multivariate_normal(mu, var(x))\n",
    "        return\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fitea los datos en la dist\n",
    "        :param X: Datos con vectores columna uno al lado del otro\n",
    "        :param y: vector vertical con los resultados\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "\n",
    "        N = len(y)\n",
    "        c_alpha = self.alpha_0\n",
    "        c_beta = self.beta_0\n",
    "        c_gamma = get_gamma(c_alpha, c_beta, X)\n",
    "        c_S_N = get_S_N(c_alpha, c_beta, X)\n",
    "        c_m_N = get_m_N(c_beta, c_S_N, X, y)\n",
    "        i = 0\n",
    "\n",
    "        while i < self.maxiter:\n",
    "\n",
    "            if i % 25 == 0:\n",
    "                print('{}/{}'.format(i, self.maxiter))\n",
    "\n",
    "            n_alpha = get_alpha(c_gamma, c_m_N)\n",
    "            n_beta = get_beta(N, c_gamma, c_m_N, X, y)\n",
    "            n_gamma = get_gamma(c_alpha, c_beta, X)\n",
    "            c_S_N = get_S_N(n_alpha, n_beta, X)\n",
    "            c_m_N = get_m_N(n_beta, c_S_N, X, y)\n",
    "\n",
    "            # convergence\n",
    "            for_conv_1 = convergence(c_alpha, n_alpha, self.tol)\n",
    "            for_conv_2 = convergence(c_beta, n_beta, self.tol)\n",
    "            for_conv_3 = convergence(c_gamma, n_gamma, self.tol)\n",
    "\n",
    "            # break condition\n",
    "            if for_conv_1 and for_conv_2 and for_conv_3:\n",
    "                break\n",
    "\n",
    "            c_alpha = n_alpha.copy()\n",
    "            c_beta = n_beta.copy()\n",
    "            c_gamma = n_gamma.copy()\n",
    "\n",
    "        self.alpha = n_alpha\n",
    "        self.beta = n_beta\n",
    "        self.gamma = n_gamma\n",
    "        self.posteriori = self.get_posteriori(X, y, self.alpha, self.beta)\n",
    "\n",
    "        return\n",
    "\n",
    "    def predict(self, X, return_std=False):\n",
    "        distribution = self.posteriori(X)\n",
    "        return distribution(X)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
